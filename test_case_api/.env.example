# Test Case Generator API Configuration

# Ollama Configuration
# URL where Ollama is running (local or remote)
OLLAMA_BASE_URL =http://localhost:11434
API_SERVER      =http://8p89b74:5000

# Ollama model to use for test case generation
# Common options: llama2, mistral, neural-chat, dolphin-mixtral, etc.
OLLAMA_MODEL=mistral:instruct

# Timeout for Ollama API calls (seconds)
OLLAMA_TIMEOUT=180

# Flask Configuration
# Host and port for the API server
HOST=0.0.0.0
PORT=5000

# Debug mode (True for development, False for production)
FLASK_DEBUG=True
DEBUG_MODE=True

# File Upload Configuration
# Maximum file size in MB
MAX_FILE_SIZE_MB=10

# Logging
# Leave empty for INFO level, set to DEBUG for verbose output
LOG_LEVEL=INFO

# Target file for batch requirements
TARGET_FILE         = samples/batch_requirements.json
TARGET_FILE_NAME    = batch_requirements.json

USE_STREAMING = True